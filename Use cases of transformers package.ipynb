{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ec01d5",
   "metadata": {},
   "source": [
    " # Présentation des cas d'utilisation des modèles complexes de NLP\n",
    " ## Exemples avec le package 'transformers'\n",
    " https://huggingface.co/docs/transformers/index\n",
    " \n",
    " ### Ici on utilise les fonctionnalités de pipelines fournies par le package transformer\n",
    " https://huggingface.co/docs/transformers/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b91210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Set up\n",
    "\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7f220",
   "metadata": {},
   "source": [
    "# Différents imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c73b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4719",
   "metadata": {},
   "source": [
    "# Initialisation du texte à traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c135ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I ordered a used book from your online site. \\\n",
    "The book is Les mystères de Paris from Victor Hugo. \\\n",
    "It was indicated on the site that it was in good condition. \\\n",
    "I received it a week after ordering. \\\n",
    "And while unpacking the package I realized that it was damaged (damaged pages, writings). \\\n",
    "I contacted customer service who proceeded to the immediate refund.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae489082",
   "metadata": {},
   "source": [
    "# Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109e122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslation_en_to_fr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m translator(text, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32md:\\Eric\\Simplon\\openai-quickstart-python\\openai\\lib\\site-packages\\transformers\\pipelines\\__init__.py:650\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    649\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 650\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    651\u001b[0m     model,\n\u001b[0;32m    652\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    653\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    654\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    655\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    656\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    657\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    658\u001b[0m )\n\u001b[0;32m    660\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    662\u001b[0m load_tokenizer \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m TOKENIZER_MAPPING \u001b[39mor\u001b[39;00m model_config\u001b[39m.\u001b[39mtokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Eric\\Simplon\\openai-quickstart-python\\openai\\lib\\site-packages\\transformers\\pipelines\\base.py:209\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    210\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m     )\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    215\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_pipeline\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "translator = pipeline('translation_en_to_fr')\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86396a14",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\")\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae3d51",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5faebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(\"Donald Trump, the former president, is now playing golf all days in Florida with his friend Bill\")\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d80a0",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "results = []\n",
    "questions = [\"What was ordered?\",\n",
    "\"What did the customer service do?\"]\n",
    "for question in questions:\n",
    "    outputs = reader(question=question, context=text)\n",
    "    results.append(outputs)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a19532",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=150, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85bdcc",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab24a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear friends. Let me tell you my last experience with this online store where I ordered a book.\"\n",
    "prompt = text + \"\\n\\nStory I told to my friends:\\n\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1767082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('openai': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "88b77bed89c1591940896ccd82fef5a7b2f82ad7768ba756f38414c7dc10558a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
